# Ollama

Execute modelos de IA localmente, como Llama 3, Gemma, Phi e Mistral, sem depender de API.

## 🚀 Recursos
- Rode LLMs diretamente no seu servidor
- Interface web integrada (Ollama WebUI)
- Download automático de modelos (Mistral, Llama 3, etc)
- Subdomínios automáticos (API e WebUI)
- SSL via Let's Encrypt
- Dados persistentes em volume Docker

## 📦 Requisitos
- Docker e Docker Compose
- 4GB de RAM (recomendado)
- 20GB de armazenamento (para modelos)
- CPU com suporte a AVX2 (recomendado)

## 🧩 Personalização
Após a instalação, acesse:
- API: `https://ollama.seusite.com`
- WebUI: `https://webgpt.seusite.com`
- Dados persistentes: `/opt/xpanel/data/ollama`

## 🧠 Modelos Disponíveis
- `llama3`
- `gemma`
- `phi`
- `mistral`
- `qwen`
- `dolphin-mistral`
- Mais em: [https://ollama.com/library](https://ollama.com/library)

## 🌐 Documentação Oficial
[https://docs.ollama.com](https://docs.ollama.com)

## 💬 Sobre o App
Este app foi integrado pela comunidade XPanel para funcionar em 1 clique, com SSL, subdomínio automático e configuração completa. É parte da **XInstala**, a loja oficial de apps do XPanel.