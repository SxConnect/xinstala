# Ollama

Execute modelos de IA localmente, como Llama 3, Gemma, Phi e Mistral, sem depender de API.

## 游 Recursos
- Rode LLMs diretamente no seu servidor
- Interface web integrada (Ollama WebUI)
- Download autom치tico de modelos (Mistral, Llama 3, etc)
- Subdom칤nios autom치ticos (API e WebUI)
- SSL via Let's Encrypt
- Dados persistentes em volume Docker

## 游닍 Requisitos
- Docker e Docker Compose
- 4GB de RAM (recomendado)
- 20GB de armazenamento (para modelos)
- CPU com suporte a AVX2 (recomendado)

## 游빌 Personaliza칞칚o
Ap칩s a instala칞칚o, acesse:
- API: `https://ollama.seusite.com`
- WebUI: `https://webgpt.seusite.com`
- Dados persistentes: `/opt/xpanel/data/ollama`

## 游 Modelos Dispon칤veis
- `llama3`
- `gemma`
- `phi`
- `mistral`
- `qwen`
- `dolphin-mistral`
- Mais em: [https://ollama.com/library](https://ollama.com/library)

## 游깷 Documenta칞칚o Oficial
[https://docs.ollama.com](https://docs.ollama.com)

## 游눫 Sobre o App
Este app foi integrado pela comunidade XPanel para funcionar em 1 clique, com SSL, subdom칤nio autom치tico e configura칞칚o completa. 칄 parte da **XInstala**, a loja oficial de apps do XPanel.